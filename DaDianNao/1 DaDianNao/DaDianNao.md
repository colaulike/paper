# DaDianNao: A Neural Network Supercomputer

## abstract

许多公司正在为消费者或行业部署大量基于机器学习算法的复杂数据处理服务。最先进、最受欢迎的机器学习算法是卷积和深度神经网络(CNNs和DNNs)，众所周知，这类网络的计算和内存密集型。最近提出了许多神经网络加速器，它们可以提供高的计算容量/面积比，但仍然受到内存访问的阻碍。然而，与处理器在通用工作负载上面临的内存墙不同，CNNs和DNNs的内存占用虽然很大，但并没有超出多芯片系统的片上存储能力。这个特性，结合CNN/DNN算法的特点，可以导致高内部带宽和低外部通信，这反过来可以以合理的面积成本实现高度并行。在这篇文章中，我们沿着这些线介绍了一个可定制的多芯片机器学习架构，并通过集成电气和光学芯片内部互连分别评估性能。我们表明，在已知的最大神经网络层的子集上，在GPU上可以实现656.63倍的加速，并在64芯片系统上平均减少184.05倍的能量。我们以28纳米实现了节点和路由，包含自定义存储和计算单元的组合，以及电子芯片间互连。

# introduction

机器学习算法已经在非常广泛的应用和云服务中变得无处不在;例如语音识别，如Siri或谷歌Now，点击预测放置广告[27]，人脸识别在苹果iPhoto或谷歌Picasa，机器人[20]，医药研究[9]等。可以毫不夸张地说，机器学习应用程序正在取代科学计算，成为高性能计算的主要驱动力。这一转变的早期表现是2005年英特尔呼吁重新关注识别,挖掘和综合应用(后来导致PARSEC基准套件[3]),识别和挖掘主要对应于机器学习任务,或IBM开发沃森超级计算机，和2011年jeopardy game一起说明[19]。

值得注意的是，与此同时，这种应用领域的深刻转变正在发生，在机器学习和硬件领域，两种同时发生的转变(尽管看起来不相关)正在发生。我们的社区很清楚异构计算的趋势，在这种趋势中，架构专门化被视为在低能耗的[21]上实现高性能的有前途的途径，只要我们能找到协调架构专门化和灵活性的方法。与此同时，自2006年以来，机器学习领域发生了深刻的发展，其中一类被称为深度学习(卷积和深度神经网络)的算法已经在[28]、[32]、[33]、[34]等广泛应用中成为最先进的算法。换句话说，当架构师需要在灵活性和效率之间找到一个好的平衡点时，结果是，只有一类算法可以用于实现广泛的应用程序。换句话说，有一个相当独特的机会来设计高度专业化和高效的硬件，这将使许多新兴的高性能应用程序受益。

一些研究小组已经开始利用这种特殊的环境设计加速器，旨在集成到异构多核中。Temam[47]提出一个多层感知器神经网络加速器,尽管它不是一个深度学习神经网络,Esmaeilzadeh等。[16]提出一个名为NPU的、使用硬件的神经网络来近似任何程序功能,虽然不是专门为机器学习应用,Chen等人[5]提出了深度学习加速器(CNN和DNN)。然而，所有这些加速器都有明显的神经网络大小限制:要么是几十个神经元组成的小型神经网络可以执行，要么是神经元和突触(即神经元之间的连接权值)中间值必须存储在主存储器中。这两个限制分别从机器学习和硬件的角度来看是严重的。

从机器学习的角度来看，有一个显著的趋势是越来越大的神经网络。Krizhevsky等人[32]最近的工作在ImageNet数据库[13]上达到了最先进的精度，“只有”6000万个参数。最近有一个10亿参数的神经网络[34]的例子，一些作者甚至在第二年研究了100亿参数的神经网络[8]。然而，这些网络目前被认为是无监督学习的极端实验(第一个在16000个cpu上，第二个在64个gpu上)，它们的性能被更小但更经典的神经网络(如Krizhevsky等人的[32])超越。然而，尽管神经网络的规模发展不可能是单调的，但有一个向更大的神经网络发展的明确趋势。此外，越来越大的输入(例如，高清代替SD图像)将进一步扩大神经网络的规模。从硬件的角度来看,上述加速器是有限的,因为如果大多数突触权重必须驻留在内存,如果神经元中间值必须经常写从内存中读取,内存访问成为性能瓶颈,就像在处理器、部分无效使用自定义架构的好处。Chen等人[5]通过观察他们的神经网络加速器由于内存访问而损失了至少一个数量级的性能，承认了这个问题。

然而,从机器学习的角度来看，尽管10亿多参数或更多的参数量很大,重要的是要意识到这一点：事实上,它不是从硬件角度看:如果每个参数需要64位,只有对应于8 GB(有明确的迹象表明更少位已经足够了)。而8 GB仍然太大,一个芯片上,可以想象一个专用的机器学习计算机组成的多个芯片,每个芯片包含专业逻辑与足够的RAM内存之和的芯片可以包含整个神经网络,不需要主存。通过一个专用的网格将这些不同的芯片紧密地连接起来，我们就可以实现现有最大的DNNs，以目前使用的许多CPU或GPU的一小部分能量和面积实现高性能。由于其低能耗和面积成本,这样的机器,一种紧凑的机器学习超级计算机,可以帮助传播使用高精度的机器学习应用程序,或相反地通过扩大RAM存储在每个节点和/或提高节点的数量来使用更大的DNN/ CNN。

在本文中，我们提出了这样一个架构，由电互连的节点组成，每个节点包含计算逻辑、eDRAM和路由器结构;节点被实现到28纳米的位置和路由，我们评估了一个多达64个节点的架构。在一个最大的现有神经网络层的样本上，我们表明在GPU上可以实现656.63倍的加速，并平均减少184.05倍的能量。

我们也评估基于硅光子学光学互联(SiPh)[54],并观察64节点系统光学互联的系统在分类器层实现2.20倍的加速,在64节点系统和电气连接上实现的最大代表性的神经网络层上平均加速1.13 倍，平均能量减少1.16 倍。

在第2部分中,我们介绍了CNN,DNN和硅光子技术,在第三节,我们评估这些发自GPU,在第四节我们比较GPU和最近提议的CNN、DNN加速器,在第5部分中,我们介绍了机器学习的超级计算机,我们提出的方法在第六节,节实验结果7和8节的相关工作。